{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a6377d80",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aae88b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration, MBartTokenizer\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fdb2f4ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "96cfcf5f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_m = 'EvaHan2023_train_data/train_24_histories_m_utf8.txt'\n",
    "train_data_c = 'EvaHan2023_train_data/train_24-historoes_c_utf8.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dfecae61",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "84e6153d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "output_dir = './model_save/mbart-large-cc25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5cbccdaf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processdata(filename_m, filename_c):\n",
    "    with open(filename_m, 'r', encoding='utf-8') as f:\n",
    "        data_m = [i.strip().split('\\n') for i in f.readlines()]\n",
    "    with open(filename_c, 'r', encoding='utf-8') as g:\n",
    "        data_c = [i.strip().split('\\n') for i in g.readlines()]\n",
    "    df = pd.DataFrame({'source':data_c, 'target':data_m})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fe121892",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, src_lang, tgt_lang, model_name, with_labels = True):\n",
    "        self.tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.with_labels = with_labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.with_labels:\n",
    "            src = self.data.loc[index,'source']\n",
    "            tgt = self.data.loc[index,'target']\n",
    "            batch = self.tokenizer.prepare_seq2seq_batch(src, tgt_texts = tgt, src_lang = self.src_lang, tgt_lang = self.tgt_lang, return_tensors=\"pt\").to(device)\n",
    "            input_ids = batch[\"input_ids\"].squeeze(0)\n",
    "            target_ids = batch[\"labels\"].squeeze(0)\n",
    "            return input_ids, target_ids\n",
    "        else:\n",
    "            src = self.data.loc[index,'source']\n",
    "            batch = self.tokenizer.prepare_seq2seq_batch(src, src_lang = self.src_lang, return_tensors=\"pt\").to(device)\n",
    "            input_ids = batch[\"input_ids\"].squeeze(0)\n",
    "            return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a76a2106",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name, freeze_bert = False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "        self.model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        if freeze_bert:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, labels):\n",
    "        output = self.model(input_ids, labels=labels)\n",
    "        return output.loss\n",
    "\n",
    "    def generate(self, input_ids, labels, decoder_start_token):\n",
    "        generated_tokens = self.model.generate(input_ids, decoder_start_token_id = self.tokenizer.lang_code_to_id[decoder_start_token])\n",
    "        generated_sentences = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        ground_truth_sentences = self.tokenizer.batch_decode(labels, skip_special_tokens=True)[0]\n",
    "        return generated_sentences, ground_truth_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0d076035",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3e7f092b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save(model, optimizer):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, model)\n",
    "    print('the best model has been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d88c0111",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, test_size=0.2, shuffle=True, random_state=None):\n",
    "    train = data[int(len(data)*test_size):].reset_index(drop=True)\n",
    "    test = data[:int(len(data)*test_size)].reset_index(drop=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "625877c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_eval(model, optimizer, train_loader, val_loader, epochs=50):\n",
    "    print('start training')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print('epoch:', epoch+1)\n",
    "        train_loss = 0\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            loss = model(batch[0], batch[1])\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Train Loss: {train_loss:.4f}\")\n",
    "        eval(model, optimizer, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ed581d0c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval(model, optimizer, val_loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(val_loader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            if batch_idx == 1:\n",
    "                generated_sentences, ground_truth_sentences = model.generate(batch[0], batch[1], 'zh_CN')\n",
    "                print('generated_sentences:', generated_sentences)\n",
    "                print('ground_truth_sentences:', ground_truth_sentences)\n",
    "            loss = model(batch[0], batch[1])\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    eval_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] - Valid Loss: {valid_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "64e8efce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/245996 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[     6, 123551,  59600,   3302,    630, 246455, 245306,      4, 123052,\n",
      "          10560,     37,  26335,     37,  41318,   1971,  29958,   3169,    635,\n",
      "          46989,     30,      2, 250012]], device='cuda:0'), tensor([[     6, 123551,  59600,   3302,    630, 246455, 245306,    635,      4,\n",
      "         123052,  10560,     37,  26335,     37,  41318,   1971,  29958,   3169,\n",
      "            635,  21381,  46989,     30,      2, 250025]], device='cuda:0'))\n",
      "0 4.659609794616699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/245996 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 6.00 GiB total capacity; 4.97 GiB already allocated; 0 bytes free; 5.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(model_name, freeze_bert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[190], line 13\u001b[0m, in \u001b[0;36mtrain_eval\u001b[1;34m(model, optimizer, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_idx, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28meval\u001b[39m(model, optimizer, val_loader)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs224n\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs224n\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 6.00 GiB total capacity; 4.97 GiB already allocated; 0 bytes free; 5.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "df = processdata(train_data_m, train_data_c)\n",
    "train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_set = CustomDataset(train, 'ja_XX', 'zh_CN', model_name)\n",
    "train_dataset = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "test_set = CustomDataset(test, 'ja_XX', 'zh_CN', model_name)\n",
    "test_dataset = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "model = MyModel(model_name, freeze_bert = False)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "train_eval(model, optimizer, train_dataset, test_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8176184d",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbba062",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf7d028",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>全新一代32位高速处理性能</h1> <h2>Intel推出intel80836 - 领先的32位微处理器</h2> <p>32位架构,大幅提升计算性能</p> <p>内存管理单元,优化系统资源利用</p> <p>支持多任务并行,效率翻倍</p> <p>丰富接口,随心扩展无限可能</p> <h2>intel80836,向高效计算的新纪元进发</h2> <p>性能强劲,拓展无限,创造属于你的专业计算体验</p> <table border=\"1\"> <tr><td>数据和地址总线宽度</td><td>32位</td></tr> <tr><td>寻址空间</td><td>物理地址4GB</td></tr> <tr><td>缓存</td><td>支持L1 Cache</td></tr> <tr><td>总线接口</td><td>支持多种外部总线接口,如ISA、PCI等</td></tr> <tr><td>内存管理</td><td>支持虚拟内存和分页机制</td></tr> <tr><td>指令集</td><td>完整支持 x86 指令集</td></tr> <tr><td>时钟频率</td><td>12.5MHz</td></tr> </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "text = '<h1>全新一代32位高速处理性能</h1> <h2>Intel推出intel80836 - 领先的32位微处理器</h2> <p>32位架构,大幅提升计算性能</p> <p>内存管理单元,优化系统资源利用</p> <p>支持多任务并行,效率翻倍</p> <p>丰富接口,随心扩展无限可能</p> <h2>intel80836,向高效计算的新纪元进发</h2> <p>性能强劲,拓展无限,创造属于你的专业计算体验</p> <table border=\"1\"> <tr><td>数据和地址总线宽度</td><td>32位</td></tr> <tr><td>寻址空间</td><td>物理地址4GB</td></tr> <tr><td>缓存</td><td>支持L1 Cache</td></tr> <tr><td>总线接口</td><td>支持多种外部总线接口,如ISA、PCI等</td></tr> <tr><td>内存管理</td><td>支持虚拟内存和分页机制</td></tr> <tr><td>指令集</td><td>完整支持 x86 指令集</td></tr> <tr><td>时钟频率</td><td>12.5MHz</td></tr> </table>'\n",
    "\n",
    "display(HTML(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1e586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
